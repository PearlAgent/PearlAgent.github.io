<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>pearl.utils.functional_utils.experimentation.create_offline_data API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pearl.utils.functional_utils.experimentation.create_offline_data</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env fbpython
# (c) Meta Platforms, Inc. and affiliates. Confidential and proprietary.

import pickle
from collections import deque
from typing import List, Optional

import torch
from pearl.api.environment import Environment
from pearl.api.reward import Value
from pearl.pearl_agent import PearlAgent
from pearl.utils.functional_utils.train_and_eval.online_learning import run_episode
from pearl.utils.instantiations.spaces.discrete_action import DiscreteActionSpace


def create_offline_data(
    agent: PearlAgent,
    env: Environment,
    save_path: str,
    file_name: str,
    max_len_offline_data: int = 50000,
    learn: bool = True,
    learn_after_episode: bool = True,
    evaluation_episodes: int = 100,
    seed: Optional[int] = None,
) -&gt; List[Value]:

    &#34;&#34;&#34;
    This function creates offline data by interacting with a given environment using a specified
    Pearl Agent. This is mostly for illustration with standard benchmark environments only.
    For most practical use cases, offline data collection will use custom pipelines.

    Args:
        agent: a pearl agent with policy learner, exploration module and replay buffer specified
               (e.g. a DQN agent).
        env: an environment to collect data from (e.g. GymEnvironment)
        number_of_episodes: number of episodes for which data is to be collected.
        learn: whether to learn after each episode (depends on the policy learner used by agent).
        exploit: set as default to False as we want exploration during data collection.
        learn_after_episode: whether to learn after each episode
        (depends on the policy learner used by agent).
    &#34;&#34;&#34;

    # much of this function overlaps with episode return function but i think writing it
    # like this is cleaner

    print(f&#34;collecting data from env: {env} using agent: {agent}&#34;)

    epi_returns = []
    epi = 0
    raw_transitions_buffer = deque([], maxlen=max_len_offline_data)
    while len(raw_transitions_buffer) &lt; max_len_offline_data:
        g = 0
        observation, action_space = env.reset(seed=seed)
        agent.reset(observation, action_space)
        done = False
        while not done:
            action = agent.act(
                exploit=False
            )  # exploit is explicitly set to False as we want exploration during data collection.
            action_result = env.step(action)
            g += action_result.reward
            agent.observe(action_result)
            transition_tuple = {
                &#34;observation&#34;: observation,
                &#34;action&#34;: action,
                &#34;reward&#34;: action_result.reward,
                &#34;next_observation&#34;: action_result.observation,
                &#34;curr_available_actions&#34;: env.action_space,
                &#34;next_available_actions&#34;: env.action_space,
                &#34;done&#34;: action_result.done,
                &#34;max_number_actions&#34;: env.action_space.n
                if isinstance(env.action_space, DiscreteActionSpace)
                else None,
            }

            observation = action_result.observation
            raw_transitions_buffer.append(transition_tuple)
            if learn and not learn_after_episode:
                agent.learn()
            done = action_result.done

        if learn and learn_after_episode:
            agent.learn()

        epi_returns.append(g)
        print(f&#34;\rEpisode {epi}, return={g}&#34;, end=&#34;&#34;)
        epi += 1

    # save offline transition tuples in a .pt file
    torch.save(raw_transitions_buffer, save_path + file_name)

    # save training returns of the data collection agent
    with open(
        save_path
        + &#34;training_returns_data_collection_agent_&#34;
        + str(max_len_offline_data)
        + &#34;.pickle&#34;,
        &#34;wb&#34;,
    ) as handle:
        # @lint-ignore PYTHONPICKLEISBAD
        pickle.dump(epi_returns, handle, protocol=pickle.HIGHEST_PROTOCOL)

    # evaluation results of the data collection agent
    print(&#34; &#34;)
    print(
        &#34;data collection complete; starting evaluation runs for data collection agent&#34;
    )

    evaluation_returns = []
    for i in range(evaluation_episodes):
        # data creation and evaluation seed should be different
        evaluation_seed = seed + i if seed is not None else seed
        episode_info, _ = run_episode(
            agent=agent,
            env=env,
            learn=False,
            exploit=True,
            learn_after_episode=False,
            seed=evaluation_seed,
        )
        g = episode_info[&#34;return&#34;]
        print(f&#34;\repisode {i}, return={g}&#34;, end=&#34;&#34;)
        evaluation_returns.append(g)

    with open(
        save_path
        + &#34;evaluation_returns_data_collection_agent_&#34;
        + str(max_len_offline_data)
        + &#34;.pickle&#34;,
        &#34;wb&#34;,
    ) as handle:
        # @lint-ignore PYTHONPICKLEISBAD
        pickle.dump(evaluation_returns, handle, protocol=pickle.HIGHEST_PROTOCOL)

    return epi_returns  # for plotting returns of the policy used to collect offine data


# getting returns of the data collection agent, either from file or by stitching trajectories
# in the training data
def get_data_collection_agent_returns(
    data_path: str,
    returns_file_path: Optional[str] = None,
) -&gt; List[Value]:

    &#34;&#34;&#34;
    This function returns episode returns of a Pearl Agent using for offline data collection.
    The returns file can be directly provided or we can stitch together trajectories in the offline
    data. This function is used to compute normalized scores for offline rl benchmarks.

    Args:
        data_path: path to the directory where the offline data is stored.
        returns_file_path: path to the file containing returns of the data collection agent.
    &#34;&#34;&#34;

    print(&#34;getting returns of the data collection agent agent&#34;)
    if returns_file_path is None:
        print(
            f&#34;using offline training data in {data_path} to stitch trajectories and compute returns&#34;
        )
        with open(data_path, &#34;rb&#34;) as file:
            data = torch.load(file, map_location=torch.device(&#34;cpu&#34;))

        data_collection_agent_returns = []
        g = 0
        for transition in list(data):
            if transition[&#34;done&#34;]:
                data_collection_agent_returns.append(g)
                g = 0
            else:
                g += transition[&#34;reward&#34;]
    else:
        print(f&#34;loading returns from file {returns_file_path}&#34;)
        with open(returns_file_path, &#34;rb&#34;) as file:
            # @lint-ignore PYTHONPICKLEISBAD
            data_collection_agent_returns = pickle.load(file)

    return data_collection_agent_returns</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pearl.utils.functional_utils.experimentation.create_offline_data.create_offline_data"><code class="name flex">
<span>def <span class="ident">create_offline_data</span></span>(<span>agent: <a title="pearl.pearl_agent.PearlAgent" href="../../../pearl_agent.html#pearl.pearl_agent.PearlAgent">PearlAgent</a>, env: <a title="pearl.api.environment.Environment" href="../../../api/environment.html#pearl.api.environment.Environment">Environment</a>, save_path: str, file_name: str, max_len_offline_data: int = 50000, learn: bool = True, learn_after_episode: bool = True, evaluation_episodes: int = 100, seed: Optional[int] = None) ‑> List[object]</span>
</code></dt>
<dd>
<div class="desc"><p>This function creates offline data by interacting with a given environment using a specified
Pearl Agent. This is mostly for illustration with standard benchmark environments only.
For most practical use cases, offline data collection will use custom pipelines.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>agent</code></strong></dt>
<dd>a pearl agent with policy learner, exploration module and replay buffer specified
(e.g. a DQN agent).</dd>
<dt><strong><code>env</code></strong></dt>
<dd>an environment to collect data from (e.g. GymEnvironment)</dd>
<dt><strong><code>number_of_episodes</code></strong></dt>
<dd>number of episodes for which data is to be collected.</dd>
<dt><strong><code>learn</code></strong></dt>
<dd>whether to learn after each episode (depends on the policy learner used by agent).</dd>
<dt><strong><code>exploit</code></strong></dt>
<dd>set as default to False as we want exploration during data collection.</dd>
<dt><strong><code>learn_after_episode</code></strong></dt>
<dd>whether to learn after each episode</dd>
</dl>
<p>(depends on the policy learner used by agent).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_offline_data(
    agent: PearlAgent,
    env: Environment,
    save_path: str,
    file_name: str,
    max_len_offline_data: int = 50000,
    learn: bool = True,
    learn_after_episode: bool = True,
    evaluation_episodes: int = 100,
    seed: Optional[int] = None,
) -&gt; List[Value]:

    &#34;&#34;&#34;
    This function creates offline data by interacting with a given environment using a specified
    Pearl Agent. This is mostly for illustration with standard benchmark environments only.
    For most practical use cases, offline data collection will use custom pipelines.

    Args:
        agent: a pearl agent with policy learner, exploration module and replay buffer specified
               (e.g. a DQN agent).
        env: an environment to collect data from (e.g. GymEnvironment)
        number_of_episodes: number of episodes for which data is to be collected.
        learn: whether to learn after each episode (depends on the policy learner used by agent).
        exploit: set as default to False as we want exploration during data collection.
        learn_after_episode: whether to learn after each episode
        (depends on the policy learner used by agent).
    &#34;&#34;&#34;

    # much of this function overlaps with episode return function but i think writing it
    # like this is cleaner

    print(f&#34;collecting data from env: {env} using agent: {agent}&#34;)

    epi_returns = []
    epi = 0
    raw_transitions_buffer = deque([], maxlen=max_len_offline_data)
    while len(raw_transitions_buffer) &lt; max_len_offline_data:
        g = 0
        observation, action_space = env.reset(seed=seed)
        agent.reset(observation, action_space)
        done = False
        while not done:
            action = agent.act(
                exploit=False
            )  # exploit is explicitly set to False as we want exploration during data collection.
            action_result = env.step(action)
            g += action_result.reward
            agent.observe(action_result)
            transition_tuple = {
                &#34;observation&#34;: observation,
                &#34;action&#34;: action,
                &#34;reward&#34;: action_result.reward,
                &#34;next_observation&#34;: action_result.observation,
                &#34;curr_available_actions&#34;: env.action_space,
                &#34;next_available_actions&#34;: env.action_space,
                &#34;done&#34;: action_result.done,
                &#34;max_number_actions&#34;: env.action_space.n
                if isinstance(env.action_space, DiscreteActionSpace)
                else None,
            }

            observation = action_result.observation
            raw_transitions_buffer.append(transition_tuple)
            if learn and not learn_after_episode:
                agent.learn()
            done = action_result.done

        if learn and learn_after_episode:
            agent.learn()

        epi_returns.append(g)
        print(f&#34;\rEpisode {epi}, return={g}&#34;, end=&#34;&#34;)
        epi += 1

    # save offline transition tuples in a .pt file
    torch.save(raw_transitions_buffer, save_path + file_name)

    # save training returns of the data collection agent
    with open(
        save_path
        + &#34;training_returns_data_collection_agent_&#34;
        + str(max_len_offline_data)
        + &#34;.pickle&#34;,
        &#34;wb&#34;,
    ) as handle:
        # @lint-ignore PYTHONPICKLEISBAD
        pickle.dump(epi_returns, handle, protocol=pickle.HIGHEST_PROTOCOL)

    # evaluation results of the data collection agent
    print(&#34; &#34;)
    print(
        &#34;data collection complete; starting evaluation runs for data collection agent&#34;
    )

    evaluation_returns = []
    for i in range(evaluation_episodes):
        # data creation and evaluation seed should be different
        evaluation_seed = seed + i if seed is not None else seed
        episode_info, _ = run_episode(
            agent=agent,
            env=env,
            learn=False,
            exploit=True,
            learn_after_episode=False,
            seed=evaluation_seed,
        )
        g = episode_info[&#34;return&#34;]
        print(f&#34;\repisode {i}, return={g}&#34;, end=&#34;&#34;)
        evaluation_returns.append(g)

    with open(
        save_path
        + &#34;evaluation_returns_data_collection_agent_&#34;
        + str(max_len_offline_data)
        + &#34;.pickle&#34;,
        &#34;wb&#34;,
    ) as handle:
        # @lint-ignore PYTHONPICKLEISBAD
        pickle.dump(evaluation_returns, handle, protocol=pickle.HIGHEST_PROTOCOL)

    return epi_returns  # for plotting returns of the policy used to collect offine data</code></pre>
</details>
</dd>
<dt id="pearl.utils.functional_utils.experimentation.create_offline_data.get_data_collection_agent_returns"><code class="name flex">
<span>def <span class="ident">get_data_collection_agent_returns</span></span>(<span>data_path: str, returns_file_path: Optional[str] = None) ‑> List[object]</span>
</code></dt>
<dd>
<div class="desc"><p>This function returns episode returns of a Pearl Agent using for offline data collection.
The returns file can be directly provided or we can stitch together trajectories in the offline
data. This function is used to compute normalized scores for offline rl benchmarks.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data_path</code></strong></dt>
<dd>path to the directory where the offline data is stored.</dd>
<dt><strong><code>returns_file_path</code></strong></dt>
<dd>path to the file containing returns of the data collection agent.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_data_collection_agent_returns(
    data_path: str,
    returns_file_path: Optional[str] = None,
) -&gt; List[Value]:

    &#34;&#34;&#34;
    This function returns episode returns of a Pearl Agent using for offline data collection.
    The returns file can be directly provided or we can stitch together trajectories in the offline
    data. This function is used to compute normalized scores for offline rl benchmarks.

    Args:
        data_path: path to the directory where the offline data is stored.
        returns_file_path: path to the file containing returns of the data collection agent.
    &#34;&#34;&#34;

    print(&#34;getting returns of the data collection agent agent&#34;)
    if returns_file_path is None:
        print(
            f&#34;using offline training data in {data_path} to stitch trajectories and compute returns&#34;
        )
        with open(data_path, &#34;rb&#34;) as file:
            data = torch.load(file, map_location=torch.device(&#34;cpu&#34;))

        data_collection_agent_returns = []
        g = 0
        for transition in list(data):
            if transition[&#34;done&#34;]:
                data_collection_agent_returns.append(g)
                g = 0
            else:
                g += transition[&#34;reward&#34;]
    else:
        print(f&#34;loading returns from file {returns_file_path}&#34;)
        with open(returns_file_path, &#34;rb&#34;) as file:
            # @lint-ignore PYTHONPICKLEISBAD
            data_collection_agent_returns = pickle.load(file)

    return data_collection_agent_returns</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pearl.utils.functional_utils.experimentation" href="index.html">pearl.utils.functional_utils.experimentation</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="pearl.utils.functional_utils.experimentation.create_offline_data.create_offline_data" href="#pearl.utils.functional_utils.experimentation.create_offline_data.create_offline_data">create_offline_data</a></code></li>
<li><code><a title="pearl.utils.functional_utils.experimentation.create_offline_data.get_data_collection_agent_returns" href="#pearl.utils.functional_utils.experimentation.create_offline_data.get_data_collection_agent_returns">get_data_collection_agent_returns</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>