<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>pearl.utils.scripts.benchmark API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pearl.utils.scripts.benchmark</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env fbpython
# (c) Meta Platforms, Inc. and affiliates. Confidential and proprietary.

import warnings
from abc import ABC, abstractmethod
from typing import Any, Dict, Iterable, List

from pearl.api.reward import Value

from pearl.utils.instantiations.spaces.discrete_action import DiscreteActionSpace

try:
    import gymnasium as gym
except ModuleNotFoundError:
    import gym

import argparse
import logging
import os
import pickle

import matplotlib.pyplot as plt
import numpy as np
from pearl.action_representation_modules.one_hot_action_representation_module import (
    OneHotActionTensorRepresentationModule,
)
from pearl.history_summarization_modules.lstm_history_summarization_module import (
    LSTMHistorySummarizationModule,
)
from pearl.pearl_agent import PearlAgent
from pearl.policy_learners.exploration_modules.common.normal_distribution_exploration import (  # noqa E501
    NormalDistributionExploration,
)
from pearl.policy_learners.sequential_decision_making.ddpg import (
    DeepDeterministicPolicyGradient,
)
from pearl.policy_learners.sequential_decision_making.deep_q_learning import (
    DeepQLearning,
)
from pearl.policy_learners.sequential_decision_making.ppo import (
    ProximalPolicyOptimization,
)
from pearl.policy_learners.sequential_decision_making.soft_actor_critic_continuous import (  # noqa E501
    ContinuousSoftActorCritic,
)
from pearl.policy_learners.sequential_decision_making.td3 import TD3
from pearl.replay_buffers.sequential_decision_making.fifo_off_policy_replay_buffer import (  # noqa E501
    FIFOOffPolicyReplayBuffer,
)
from pearl.replay_buffers.sequential_decision_making.on_policy_episodic_replay_buffer import (  # noqa E501
    OnPolicyEpisodicReplayBuffer,
)

from pearl.user_envs.wrappers.gym_avg_torque_cost import GymAvgTorqueWrapper
from pearl.utils.functional_utils.experimentation.set_seed import set_seed
from pearl.utils.functional_utils.train_and_eval.online_learning import online_learning
from pearl.utils.instantiations.environments.gym_environment import GymEnvironment

warnings.filterwarnings(&#34;ignore&#34;)

number_of_episodes = 2000
save_path = &#34;../fbsource/fbcode/pearl/&#34;


class Evaluation(ABC):
    &#34;&#34;&#34;
    Evaluation of an RL method on a given gym environment.
    Args:
        gym_environment_name: name of the gym environment to be evaluated
        *args: arguments passed to the constructor of the gym environment
        **kwargs: keyword arguments passed to the constructor of the gym environment
    &#34;&#34;&#34;

    def __init__(
        self,
        gym_environment_name: str,
        device_id: int,
        *args: Any,
        **kwargs: Any,
    ) -&gt; None:
        self.gym_environment_name: str = gym_environment_name
        self.device_id: int = device_id
        self.args: Any = args  # pyre-ignore
        self.kwargs: Any = kwargs  # pyre-ignore

    @abstractmethod
    def evaluate(self, seed: int = 0) -&gt; Iterable[Value]:
        &#34;&#34;&#34;Runs evaluation and returns sequence of obtained returns during training&#34;&#34;&#34;
        pass


class PearlDQN(Evaluation):
    def __init__(
        self,
        gym_environment_name: str,
        device_id: int,
        *args: Any,
        **kwargs: Any,
    ) -&gt; None:
        super(PearlDQN, self).__init__(gym_environment_name, device_id, *args, **kwargs)

    def evaluate(self, seed: int) -&gt; Iterable[Value]:
        env = GymEnvironment(self.gym_environment_name, *self.args, **self.kwargs)
        assert isinstance(env.action_space, DiscreteActionSpace)
        action_representation_module = OneHotActionTensorRepresentationModule(
            max_number_actions=env.action_space.n
        )
        agent = PearlAgent(
            policy_learner=DeepQLearning(
                state_dim=env.observation_space.shape[0],
                action_space=env.action_space,
                hidden_dims=[64, 64],
                training_rounds=20,
                action_representation_module=action_representation_module,
            ),
            replay_buffer=FIFOOffPolicyReplayBuffer(10_000),
            device_id=self.device_id,
        )
        info = online_learning(
            agent,
            env,
            number_of_episodes=number_of_episodes,
            learn_after_episode=True,
            print_every_x_episodes=1,
        )
        return info[&#34;return&#34;]


class PearlLSTMDQN(Evaluation):
    def __init__(
        self,
        gym_environment_name: str,
        device_id: int,
        *args: Any,
        **kwargs: Any,
    ) -&gt; None:
        super(PearlDQN, self).__init__(gym_environment_name, device_id, *args, **kwargs)

    def evaluate(self, seed: int) -&gt; Iterable[Value]:
        env = GymEnvironment(self.gym_environment_name, *self.args, **self.kwargs)
        hidden_dim = 8
        action_space = env.action_space
        assert isinstance(action_space, DiscreteActionSpace)
        action_representation_module = OneHotActionTensorRepresentationModule(
            max_number_actions=action_space.n
        )
        history_summarization_module = LSTMHistorySummarizationModule(
            observation_dim=env.observation_space.shape[0],
            action_dim=action_space.n,
            hidden_dim=hidden_dim,
        )
        agent = PearlAgent(
            policy_learner=DeepQLearning(
                state_dim=hidden_dim,
                action_space=env.action_space,
                hidden_dims=[64, 64],
                training_rounds=20,
                action_representation_module=action_representation_module,
            ),
            history_summarization_module=history_summarization_module,
            replay_buffer=FIFOOffPolicyReplayBuffer(10_000),
            device_id=self.device_id,
        )
        info = online_learning(
            agent,
            env,
            number_of_episodes=number_of_episodes,
            learn_after_episode=True,
            print_every_x_episodes=1,
        )
        return info[&#34;return&#34;]


class PearlContinuousSAC(Evaluation):
    def __init__(
        self,
        gym_environment_name: str,
        device_id: int,
        *args: Any,
        **kwargs: Any,
    ) -&gt; None:
        super(PearlContinuousSAC, self).__init__(
            gym_environment_name, device_id, *args, **kwargs
        )

    def evaluate(self, seed: int) -&gt; Iterable[Value]:
        env = GymEnvironment(self.gym_environment_name, *self.args, **self.kwargs)
        agent = PearlAgent(
            policy_learner=ContinuousSoftActorCritic(
                state_dim=env.observation_space.shape[0],
                action_space=env.action_space,
                actor_hidden_dims=[256, 256],
                critic_hidden_dims=[256, 256],
                training_rounds=1,
                batch_size=256,
                entropy_coef=0.05,
                entropy_autotune=True,
                actor_learning_rate=0.0003,
                critic_learning_rate=0.0005,
            ),
            replay_buffer=FIFOOffPolicyReplayBuffer(100000),
            device_id=self.device_id,
        )
        info = online_learning(
            agent,
            env,
            number_of_episodes=number_of_episodes,
            learn_after_episode=False,
            print_every_x_episodes=1,
        )
        return info[&#34;return&#34;]


class PearlPPO(Evaluation):
    def __init__(
        self,
        gym_environment_name: str,
        device_id: int,
        *args: Any,
        **kwargs: Any,
    ) -&gt; None:
        super(PearlPPO, self).__init__(gym_environment_name, device_id, *args, **kwargs)

    def evaluate(self, seed: int) -&gt; Iterable[Value]:
        env = GymEnvironment(self.gym_environment_name, *self.args, **self.kwargs)
        assert isinstance(env.action_space, DiscreteActionSpace)
        action_representation_module = OneHotActionTensorRepresentationModule(
            max_number_actions=env.action_space.n
        )
        agent = PearlAgent(
            policy_learner=ProximalPolicyOptimization(
                state_dim=env.observation_space.shape[0],
                action_space=env.action_space,
                actor_hidden_dims=[64, 64],
                critic_hidden_dims=[64, 64],
                training_rounds=50,
                batch_size=64,
                epsilon=0.1,
                action_representation_module=action_representation_module,
            ),
            replay_buffer=OnPolicyEpisodicReplayBuffer(10_000),
            device_id=self.device_id,
        )
        info = online_learning(
            agent,
            env,
            number_of_episodes=number_of_episodes,
            learn_after_episode=True,
            print_every_x_episodes=1,
        )
        return info[&#34;return&#34;]


class PearlDDPG(Evaluation):
    def __init__(
        self,
        gym_environment_name: str,
        device_id: int,
        *args: Any,
        **kwargs: Any,
    ) -&gt; None:
        super(PearlDDPG, self).__init__(
            gym_environment_name, device_id, *args, **kwargs
        )

    def evaluate(self, seed: int) -&gt; Iterable[Value]:
        env = GymEnvironment(self.gym_environment_name, *self.args, **self.kwargs)
        agent = PearlAgent(
            policy_learner=DeepDeterministicPolicyGradient(
                state_dim=env.observation_space.shape[0],
                action_space=env.action_space,
                actor_hidden_dims=[256, 256],
                critic_hidden_dims=[256, 256],
                critic_learning_rate=3e-4,
                actor_learning_rate=3e-4,
                training_rounds=1,
                exploration_module=NormalDistributionExploration(
                    mean=0,
                    std_dev=0.1,
                ),
            ),
            replay_buffer=FIFOOffPolicyReplayBuffer(50000),
            device_id=self.device_id,
        )
        info = online_learning(
            agent,
            env,
            number_of_episodes=number_of_episodes,
            learn_after_episode=True,
            print_every_x_episodes=1,
        )
        return info[&#34;return&#34;]


class PearlTD3(Evaluation):
    def __init__(
        self,
        gym_environment_name: str,
        device_id: int,
        *args: Any,
        **kwargs: Any,
    ) -&gt; None:
        super(PearlTD3, self).__init__(gym_environment_name, device_id, *args, **kwargs)

    def evaluate(self, seed: int) -&gt; Iterable[Value]:
        has_cost_available = False
        if self.gym_environment_name[:3] == &#34;wc_&#34;:
            has_cost_available = True
            self.gym_environment_name = self.gym_environment_name[3:]
            env = GymEnvironment(
                GymAvgTorqueWrapper(gym.make(self.gym_environment_name))
            )
        else:
            env = GymEnvironment(self.gym_environment_name, *self.args, **self.kwargs)
        agent = PearlAgent(
            policy_learner=TD3(
                state_dim=env.observation_space.shape[0],
                action_space=env.action_space,
                actor_hidden_dims=[256, 256],
                critic_hidden_dims=[256, 256],
                critic_learning_rate=3e-4,
                actor_learning_rate=3e-4,
                training_rounds=1,
                exploration_module=NormalDistributionExploration(
                    mean=0,
                    std_dev=0.1,
                ),
            ),
            replay_buffer=FIFOOffPolicyReplayBuffer(50000),
            device_id=self.device_id,
        )
        # Enable saving cost in replay buffer if cost is available
        agent.replay_buffer.has_cost_available = has_cost_available

        info = online_learning(
            agent,
            env,
            number_of_episodes=number_of_episodes,
            learn_after_episode=True,
            print_every_x_episodes=1,
        )
        return info[&#34;return&#34;]


def evaluate(evaluations: Iterable[Evaluation]) -&gt; None:
    &#34;&#34;&#34;Obtain data from evaluations and plot them, one plot per environment&#34;&#34;&#34;
    num_seeds = 5
    for seed in range(num_seeds):
        set_seed(seed)  # seed all sources of randomness except the envronment reset
        print(f&#34;Seed {seed}&#34;)
        data_by_environment_and_method = collect_data(evaluations, seed=seed)
        generate_plots(data_by_environment_and_method, seed=seed)


def collect_data(
    evaluations: Iterable[Evaluation], seed: int
) -&gt; Dict[str, Dict[str, List[float]]]:
    data_by_environment_and_method = {}
    for evaluation in evaluations:
        method = type(evaluation).__name__
        environment = evaluation.gym_environment_name
        print(f&#34;Running {method} on {environment} ...&#34;)
        returns = evaluation.evaluate(seed=seed)  # to set the environment seed
        if environment not in data_by_environment_and_method:
            data_by_environment_and_method[environment] = {}
        data_by_environment_and_method[environment][method] = returns
        dir_name = save_path + str(method) + &#34;/&#34; + str(environment) + &#34;/&#34;
        os.makedirs(dir_name, exist_ok=True)
        with open(
            dir_name + &#34;returns_data_seed_&#34; + str(seed) + &#34;.pickle&#34;, &#34;wb&#34;
        ) as handle:
            # @lint-ignore PYTHONPICKLEISBAD
            pickle.dump(
                data_by_environment_and_method, handle, protocol=pickle.HIGHEST_PROTOCOL
            )
    return data_by_environment_and_method


def generate_plots(
    data_by_environment_and_method: Dict[str, Dict[str, List[float]]],
    seed: int,
) -&gt; None:
    for environment_name, data_by_method in data_by_environment_and_method.items():
        plt.title(environment_name)
        plt.xlabel(&#34;Episode&#34;)
        plt.ylabel(&#34;Return&#34;)
        method, _ = next(iter(data_by_method.items()))
        window_size = 10

        for method, returns in data_by_method.items():
            plt.plot(returns, label=method)
            rolling_mean_returns = (
                np.convolve(returns, np.ones(window_size), &#34;valid&#34;) / window_size
            )

            plt.plot(rolling_mean_returns, label=f&#34;Rolling Mean {method}&#34;)
        plt.legend()
        dir_name = save_path + str(method) + &#34;/&#34; + str(environment_name) + &#34;/&#34;
        os.makedirs(dir_name, exist_ok=True)
        filename = f&#34;{environment_name} and {method} and seed = {seed}.png&#34;
        logging.info(f&#34;Saving plot to {os.getcwd()}/{filename}&#34;)
        plt.savefig(filename)
        plt.savefig(dir_name + &#34;/&#34; + filename)
        plt.close()


def main(device_id: int = -1) -&gt; None:
    # TODO: this should be part of argparse instead of hardcoded.
    evaluate(
        [
            # Methods applied to the same environment will be grouped in the same plot.
            # All receive device id
            PearlDQN(&#34;CartPole-v1&#34;, device_id=device_id),
            # PearlPPO(&#34;CartPole-v1&#34;, device_id=device_id),
            # PearlDQN(&#34;Acrobot-v1&#34;, device_id=device_id),
            # PearlPPO(&#34;Acrobot-v1&#34;, device_id=device_id),
            # PearlDDPG(&#34;Pendulum-v1&#34;, device_id=device_id),
            # PearlTD3(&#34;wc_Pendulum-v1&#34;, device_id=device_id),
            # MuJoCo environments -- require MuJoCo to be installed.
            # PearlDDPG(&#34;HalfCheetah-v4&#34;),
            # PearlDDPG(&#34;Ant-v4&#34;),
            # PearlDDPG(&#34;Hopper-v4&#34;)
            # PearlDDPG(&#34;Walker2d-v4&#34;)
            # PearlTD3(&#34;HalfCheetah-v4&#34;),
            # PearlTD3(&#34;Ant-v4&#34;),
            # PearlTD3(&#34;Hopper-v4&#34;),
            # PearlTD3(&#34;Walker2d-v4&#34;),
            # PearlContinuousSAC(&#34;Ant-v4&#34;)
        ]
    )

    # tianshou_dqn_cart_pole()
    # tianshou_ppo_cart_pole()


if __name__ == &#34;__main__&#34;:
    parser = argparse.ArgumentParser(description=&#34;Pearl Benchmark&#34;)
    parser.add_argument(
        &#34;-d&#34;,
        &#34;--device&#34;,
        help=&#34;GPU device ID (optional)&#34;,
        required=False,
        type=int,
        default=-1,
    )
    args: argparse.Namespace = parser.parse_args()

    if args.device != -1:
        print(f&#34;Going to attempt to use GPU (cuda:{args.device})&#34;)
    else:
        print(&#34;Going to attempt to use CPU&#34;)

    main(args.device)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pearl.utils.scripts.benchmark.collect_data"><code class="name flex">
<span>def <span class="ident">collect_data</span></span>(<span>evaluations: Iterable[<a title="pearl.utils.scripts.benchmark.Evaluation" href="#pearl.utils.scripts.benchmark.Evaluation">Evaluation</a>], seed: int) ‑> Dict[str, Dict[str, List[float]]]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def collect_data(
    evaluations: Iterable[Evaluation], seed: int
) -&gt; Dict[str, Dict[str, List[float]]]:
    data_by_environment_and_method = {}
    for evaluation in evaluations:
        method = type(evaluation).__name__
        environment = evaluation.gym_environment_name
        print(f&#34;Running {method} on {environment} ...&#34;)
        returns = evaluation.evaluate(seed=seed)  # to set the environment seed
        if environment not in data_by_environment_and_method:
            data_by_environment_and_method[environment] = {}
        data_by_environment_and_method[environment][method] = returns
        dir_name = save_path + str(method) + &#34;/&#34; + str(environment) + &#34;/&#34;
        os.makedirs(dir_name, exist_ok=True)
        with open(
            dir_name + &#34;returns_data_seed_&#34; + str(seed) + &#34;.pickle&#34;, &#34;wb&#34;
        ) as handle:
            # @lint-ignore PYTHONPICKLEISBAD
            pickle.dump(
                data_by_environment_and_method, handle, protocol=pickle.HIGHEST_PROTOCOL
            )
    return data_by_environment_and_method</code></pre>
</details>
</dd>
<dt id="pearl.utils.scripts.benchmark.evaluate"><code class="name flex">
<span>def <span class="ident">evaluate</span></span>(<span>evaluations: Iterable[<a title="pearl.utils.scripts.benchmark.Evaluation" href="#pearl.utils.scripts.benchmark.Evaluation">Evaluation</a>]) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Obtain data from evaluations and plot them, one plot per environment</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate(evaluations: Iterable[Evaluation]) -&gt; None:
    &#34;&#34;&#34;Obtain data from evaluations and plot them, one plot per environment&#34;&#34;&#34;
    num_seeds = 5
    for seed in range(num_seeds):
        set_seed(seed)  # seed all sources of randomness except the envronment reset
        print(f&#34;Seed {seed}&#34;)
        data_by_environment_and_method = collect_data(evaluations, seed=seed)
        generate_plots(data_by_environment_and_method, seed=seed)</code></pre>
</details>
</dd>
<dt id="pearl.utils.scripts.benchmark.generate_plots"><code class="name flex">
<span>def <span class="ident">generate_plots</span></span>(<span>data_by_environment_and_method: Dict[str, Dict[str, List[float]]], seed: int) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_plots(
    data_by_environment_and_method: Dict[str, Dict[str, List[float]]],
    seed: int,
) -&gt; None:
    for environment_name, data_by_method in data_by_environment_and_method.items():
        plt.title(environment_name)
        plt.xlabel(&#34;Episode&#34;)
        plt.ylabel(&#34;Return&#34;)
        method, _ = next(iter(data_by_method.items()))
        window_size = 10

        for method, returns in data_by_method.items():
            plt.plot(returns, label=method)
            rolling_mean_returns = (
                np.convolve(returns, np.ones(window_size), &#34;valid&#34;) / window_size
            )

            plt.plot(rolling_mean_returns, label=f&#34;Rolling Mean {method}&#34;)
        plt.legend()
        dir_name = save_path + str(method) + &#34;/&#34; + str(environment_name) + &#34;/&#34;
        os.makedirs(dir_name, exist_ok=True)
        filename = f&#34;{environment_name} and {method} and seed = {seed}.png&#34;
        logging.info(f&#34;Saving plot to {os.getcwd()}/{filename}&#34;)
        plt.savefig(filename)
        plt.savefig(dir_name + &#34;/&#34; + filename)
        plt.close()</code></pre>
</details>
</dd>
<dt id="pearl.utils.scripts.benchmark.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>device_id: int = -1) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def main(device_id: int = -1) -&gt; None:
    # TODO: this should be part of argparse instead of hardcoded.
    evaluate(
        [
            # Methods applied to the same environment will be grouped in the same plot.
            # All receive device id
            PearlDQN(&#34;CartPole-v1&#34;, device_id=device_id),
            # PearlPPO(&#34;CartPole-v1&#34;, device_id=device_id),
            # PearlDQN(&#34;Acrobot-v1&#34;, device_id=device_id),
            # PearlPPO(&#34;Acrobot-v1&#34;, device_id=device_id),
            # PearlDDPG(&#34;Pendulum-v1&#34;, device_id=device_id),
            # PearlTD3(&#34;wc_Pendulum-v1&#34;, device_id=device_id),
            # MuJoCo environments -- require MuJoCo to be installed.
            # PearlDDPG(&#34;HalfCheetah-v4&#34;),
            # PearlDDPG(&#34;Ant-v4&#34;),
            # PearlDDPG(&#34;Hopper-v4&#34;)
            # PearlDDPG(&#34;Walker2d-v4&#34;)
            # PearlTD3(&#34;HalfCheetah-v4&#34;),
            # PearlTD3(&#34;Ant-v4&#34;),
            # PearlTD3(&#34;Hopper-v4&#34;),
            # PearlTD3(&#34;Walker2d-v4&#34;),
            # PearlContinuousSAC(&#34;Ant-v4&#34;)
        ]
    )

    # tianshou_dqn_cart_pole()
    # tianshou_ppo_cart_pole()</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pearl.utils.scripts.benchmark.Evaluation"><code class="flex name class">
<span>class <span class="ident">Evaluation</span></span>
<span>(</span><span>gym_environment_name: str, device_id: int, *args: Any, **kwargs: Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluation of an RL method on a given gym environment.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>gym_environment_name</code></strong></dt>
<dd>name of the gym environment to be evaluated</dd>
<dt><strong><code>*args</code></strong></dt>
<dd>arguments passed to the constructor of the gym environment</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>keyword arguments passed to the constructor of the gym environment</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Evaluation(ABC):
    &#34;&#34;&#34;
    Evaluation of an RL method on a given gym environment.
    Args:
        gym_environment_name: name of the gym environment to be evaluated
        *args: arguments passed to the constructor of the gym environment
        **kwargs: keyword arguments passed to the constructor of the gym environment
    &#34;&#34;&#34;

    def __init__(
        self,
        gym_environment_name: str,
        device_id: int,
        *args: Any,
        **kwargs: Any,
    ) -&gt; None:
        self.gym_environment_name: str = gym_environment_name
        self.device_id: int = device_id
        self.args: Any = args  # pyre-ignore
        self.kwargs: Any = kwargs  # pyre-ignore

    @abstractmethod
    def evaluate(self, seed: int = 0) -&gt; Iterable[Value]:
        &#34;&#34;&#34;Runs evaluation and returns sequence of obtained returns during training&#34;&#34;&#34;
        pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="pearl.utils.scripts.benchmark.PearlContinuousSAC" href="#pearl.utils.scripts.benchmark.PearlContinuousSAC">PearlContinuousSAC</a></li>
<li><a title="pearl.utils.scripts.benchmark.PearlDDPG" href="#pearl.utils.scripts.benchmark.PearlDDPG">PearlDDPG</a></li>
<li><a title="pearl.utils.scripts.benchmark.PearlDQN" href="#pearl.utils.scripts.benchmark.PearlDQN">PearlDQN</a></li>
<li><a title="pearl.utils.scripts.benchmark.PearlLSTMDQN" href="#pearl.utils.scripts.benchmark.PearlLSTMDQN">PearlLSTMDQN</a></li>
<li><a title="pearl.utils.scripts.benchmark.PearlPPO" href="#pearl.utils.scripts.benchmark.PearlPPO">PearlPPO</a></li>
<li><a title="pearl.utils.scripts.benchmark.PearlTD3" href="#pearl.utils.scripts.benchmark.PearlTD3">PearlTD3</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="pearl.utils.scripts.benchmark.Evaluation.evaluate"><code class="name flex">
<span>def <span class="ident">evaluate</span></span>(<span>self, seed: int = 0) ‑> Iterable[object]</span>
</code></dt>
<dd>
<div class="desc"><p>Runs evaluation and returns sequence of obtained returns during training</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def evaluate(self, seed: int = 0) -&gt; Iterable[Value]:
    &#34;&#34;&#34;Runs evaluation and returns sequence of obtained returns during training&#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pearl.utils.scripts.benchmark.PearlContinuousSAC"><code class="flex name class">
<span>class <span class="ident">PearlContinuousSAC</span></span>
<span>(</span><span>gym_environment_name: str, device_id: int, *args: Any, **kwargs: Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluation of an RL method on a given gym environment.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>gym_environment_name</code></strong></dt>
<dd>name of the gym environment to be evaluated</dd>
<dt><strong><code>*args</code></strong></dt>
<dd>arguments passed to the constructor of the gym environment</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>keyword arguments passed to the constructor of the gym environment</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PearlContinuousSAC(Evaluation):
    def __init__(
        self,
        gym_environment_name: str,
        device_id: int,
        *args: Any,
        **kwargs: Any,
    ) -&gt; None:
        super(PearlContinuousSAC, self).__init__(
            gym_environment_name, device_id, *args, **kwargs
        )

    def evaluate(self, seed: int) -&gt; Iterable[Value]:
        env = GymEnvironment(self.gym_environment_name, *self.args, **self.kwargs)
        agent = PearlAgent(
            policy_learner=ContinuousSoftActorCritic(
                state_dim=env.observation_space.shape[0],
                action_space=env.action_space,
                actor_hidden_dims=[256, 256],
                critic_hidden_dims=[256, 256],
                training_rounds=1,
                batch_size=256,
                entropy_coef=0.05,
                entropy_autotune=True,
                actor_learning_rate=0.0003,
                critic_learning_rate=0.0005,
            ),
            replay_buffer=FIFOOffPolicyReplayBuffer(100000),
            device_id=self.device_id,
        )
        info = online_learning(
            agent,
            env,
            number_of_episodes=number_of_episodes,
            learn_after_episode=False,
            print_every_x_episodes=1,
        )
        return info[&#34;return&#34;]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pearl.utils.scripts.benchmark.Evaluation" href="#pearl.utils.scripts.benchmark.Evaluation">Evaluation</a></li>
<li>abc.ABC</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="pearl.utils.scripts.benchmark.Evaluation" href="#pearl.utils.scripts.benchmark.Evaluation">Evaluation</a></b></code>:
<ul class="hlist">
<li><code><a title="pearl.utils.scripts.benchmark.Evaluation.evaluate" href="#pearl.utils.scripts.benchmark.Evaluation.evaluate">evaluate</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="pearl.utils.scripts.benchmark.PearlDDPG"><code class="flex name class">
<span>class <span class="ident">PearlDDPG</span></span>
<span>(</span><span>gym_environment_name: str, device_id: int, *args: Any, **kwargs: Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluation of an RL method on a given gym environment.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>gym_environment_name</code></strong></dt>
<dd>name of the gym environment to be evaluated</dd>
<dt><strong><code>*args</code></strong></dt>
<dd>arguments passed to the constructor of the gym environment</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>keyword arguments passed to the constructor of the gym environment</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PearlDDPG(Evaluation):
    def __init__(
        self,
        gym_environment_name: str,
        device_id: int,
        *args: Any,
        **kwargs: Any,
    ) -&gt; None:
        super(PearlDDPG, self).__init__(
            gym_environment_name, device_id, *args, **kwargs
        )

    def evaluate(self, seed: int) -&gt; Iterable[Value]:
        env = GymEnvironment(self.gym_environment_name, *self.args, **self.kwargs)
        agent = PearlAgent(
            policy_learner=DeepDeterministicPolicyGradient(
                state_dim=env.observation_space.shape[0],
                action_space=env.action_space,
                actor_hidden_dims=[256, 256],
                critic_hidden_dims=[256, 256],
                critic_learning_rate=3e-4,
                actor_learning_rate=3e-4,
                training_rounds=1,
                exploration_module=NormalDistributionExploration(
                    mean=0,
                    std_dev=0.1,
                ),
            ),
            replay_buffer=FIFOOffPolicyReplayBuffer(50000),
            device_id=self.device_id,
        )
        info = online_learning(
            agent,
            env,
            number_of_episodes=number_of_episodes,
            learn_after_episode=True,
            print_every_x_episodes=1,
        )
        return info[&#34;return&#34;]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pearl.utils.scripts.benchmark.Evaluation" href="#pearl.utils.scripts.benchmark.Evaluation">Evaluation</a></li>
<li>abc.ABC</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="pearl.utils.scripts.benchmark.Evaluation" href="#pearl.utils.scripts.benchmark.Evaluation">Evaluation</a></b></code>:
<ul class="hlist">
<li><code><a title="pearl.utils.scripts.benchmark.Evaluation.evaluate" href="#pearl.utils.scripts.benchmark.Evaluation.evaluate">evaluate</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="pearl.utils.scripts.benchmark.PearlDQN"><code class="flex name class">
<span>class <span class="ident">PearlDQN</span></span>
<span>(</span><span>gym_environment_name: str, device_id: int, *args: Any, **kwargs: Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluation of an RL method on a given gym environment.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>gym_environment_name</code></strong></dt>
<dd>name of the gym environment to be evaluated</dd>
<dt><strong><code>*args</code></strong></dt>
<dd>arguments passed to the constructor of the gym environment</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>keyword arguments passed to the constructor of the gym environment</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PearlDQN(Evaluation):
    def __init__(
        self,
        gym_environment_name: str,
        device_id: int,
        *args: Any,
        **kwargs: Any,
    ) -&gt; None:
        super(PearlDQN, self).__init__(gym_environment_name, device_id, *args, **kwargs)

    def evaluate(self, seed: int) -&gt; Iterable[Value]:
        env = GymEnvironment(self.gym_environment_name, *self.args, **self.kwargs)
        assert isinstance(env.action_space, DiscreteActionSpace)
        action_representation_module = OneHotActionTensorRepresentationModule(
            max_number_actions=env.action_space.n
        )
        agent = PearlAgent(
            policy_learner=DeepQLearning(
                state_dim=env.observation_space.shape[0],
                action_space=env.action_space,
                hidden_dims=[64, 64],
                training_rounds=20,
                action_representation_module=action_representation_module,
            ),
            replay_buffer=FIFOOffPolicyReplayBuffer(10_000),
            device_id=self.device_id,
        )
        info = online_learning(
            agent,
            env,
            number_of_episodes=number_of_episodes,
            learn_after_episode=True,
            print_every_x_episodes=1,
        )
        return info[&#34;return&#34;]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pearl.utils.scripts.benchmark.Evaluation" href="#pearl.utils.scripts.benchmark.Evaluation">Evaluation</a></li>
<li>abc.ABC</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="pearl.utils.scripts.benchmark.Evaluation" href="#pearl.utils.scripts.benchmark.Evaluation">Evaluation</a></b></code>:
<ul class="hlist">
<li><code><a title="pearl.utils.scripts.benchmark.Evaluation.evaluate" href="#pearl.utils.scripts.benchmark.Evaluation.evaluate">evaluate</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="pearl.utils.scripts.benchmark.PearlLSTMDQN"><code class="flex name class">
<span>class <span class="ident">PearlLSTMDQN</span></span>
<span>(</span><span>gym_environment_name: str, device_id: int, *args: Any, **kwargs: Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluation of an RL method on a given gym environment.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>gym_environment_name</code></strong></dt>
<dd>name of the gym environment to be evaluated</dd>
<dt><strong><code>*args</code></strong></dt>
<dd>arguments passed to the constructor of the gym environment</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>keyword arguments passed to the constructor of the gym environment</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PearlLSTMDQN(Evaluation):
    def __init__(
        self,
        gym_environment_name: str,
        device_id: int,
        *args: Any,
        **kwargs: Any,
    ) -&gt; None:
        super(PearlDQN, self).__init__(gym_environment_name, device_id, *args, **kwargs)

    def evaluate(self, seed: int) -&gt; Iterable[Value]:
        env = GymEnvironment(self.gym_environment_name, *self.args, **self.kwargs)
        hidden_dim = 8
        action_space = env.action_space
        assert isinstance(action_space, DiscreteActionSpace)
        action_representation_module = OneHotActionTensorRepresentationModule(
            max_number_actions=action_space.n
        )
        history_summarization_module = LSTMHistorySummarizationModule(
            observation_dim=env.observation_space.shape[0],
            action_dim=action_space.n,
            hidden_dim=hidden_dim,
        )
        agent = PearlAgent(
            policy_learner=DeepQLearning(
                state_dim=hidden_dim,
                action_space=env.action_space,
                hidden_dims=[64, 64],
                training_rounds=20,
                action_representation_module=action_representation_module,
            ),
            history_summarization_module=history_summarization_module,
            replay_buffer=FIFOOffPolicyReplayBuffer(10_000),
            device_id=self.device_id,
        )
        info = online_learning(
            agent,
            env,
            number_of_episodes=number_of_episodes,
            learn_after_episode=True,
            print_every_x_episodes=1,
        )
        return info[&#34;return&#34;]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pearl.utils.scripts.benchmark.Evaluation" href="#pearl.utils.scripts.benchmark.Evaluation">Evaluation</a></li>
<li>abc.ABC</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="pearl.utils.scripts.benchmark.Evaluation" href="#pearl.utils.scripts.benchmark.Evaluation">Evaluation</a></b></code>:
<ul class="hlist">
<li><code><a title="pearl.utils.scripts.benchmark.Evaluation.evaluate" href="#pearl.utils.scripts.benchmark.Evaluation.evaluate">evaluate</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="pearl.utils.scripts.benchmark.PearlPPO"><code class="flex name class">
<span>class <span class="ident">PearlPPO</span></span>
<span>(</span><span>gym_environment_name: str, device_id: int, *args: Any, **kwargs: Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluation of an RL method on a given gym environment.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>gym_environment_name</code></strong></dt>
<dd>name of the gym environment to be evaluated</dd>
<dt><strong><code>*args</code></strong></dt>
<dd>arguments passed to the constructor of the gym environment</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>keyword arguments passed to the constructor of the gym environment</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PearlPPO(Evaluation):
    def __init__(
        self,
        gym_environment_name: str,
        device_id: int,
        *args: Any,
        **kwargs: Any,
    ) -&gt; None:
        super(PearlPPO, self).__init__(gym_environment_name, device_id, *args, **kwargs)

    def evaluate(self, seed: int) -&gt; Iterable[Value]:
        env = GymEnvironment(self.gym_environment_name, *self.args, **self.kwargs)
        assert isinstance(env.action_space, DiscreteActionSpace)
        action_representation_module = OneHotActionTensorRepresentationModule(
            max_number_actions=env.action_space.n
        )
        agent = PearlAgent(
            policy_learner=ProximalPolicyOptimization(
                state_dim=env.observation_space.shape[0],
                action_space=env.action_space,
                actor_hidden_dims=[64, 64],
                critic_hidden_dims=[64, 64],
                training_rounds=50,
                batch_size=64,
                epsilon=0.1,
                action_representation_module=action_representation_module,
            ),
            replay_buffer=OnPolicyEpisodicReplayBuffer(10_000),
            device_id=self.device_id,
        )
        info = online_learning(
            agent,
            env,
            number_of_episodes=number_of_episodes,
            learn_after_episode=True,
            print_every_x_episodes=1,
        )
        return info[&#34;return&#34;]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pearl.utils.scripts.benchmark.Evaluation" href="#pearl.utils.scripts.benchmark.Evaluation">Evaluation</a></li>
<li>abc.ABC</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="pearl.utils.scripts.benchmark.Evaluation" href="#pearl.utils.scripts.benchmark.Evaluation">Evaluation</a></b></code>:
<ul class="hlist">
<li><code><a title="pearl.utils.scripts.benchmark.Evaluation.evaluate" href="#pearl.utils.scripts.benchmark.Evaluation.evaluate">evaluate</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="pearl.utils.scripts.benchmark.PearlTD3"><code class="flex name class">
<span>class <span class="ident">PearlTD3</span></span>
<span>(</span><span>gym_environment_name: str, device_id: int, *args: Any, **kwargs: Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluation of an RL method on a given gym environment.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>gym_environment_name</code></strong></dt>
<dd>name of the gym environment to be evaluated</dd>
<dt><strong><code>*args</code></strong></dt>
<dd>arguments passed to the constructor of the gym environment</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>keyword arguments passed to the constructor of the gym environment</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PearlTD3(Evaluation):
    def __init__(
        self,
        gym_environment_name: str,
        device_id: int,
        *args: Any,
        **kwargs: Any,
    ) -&gt; None:
        super(PearlTD3, self).__init__(gym_environment_name, device_id, *args, **kwargs)

    def evaluate(self, seed: int) -&gt; Iterable[Value]:
        has_cost_available = False
        if self.gym_environment_name[:3] == &#34;wc_&#34;:
            has_cost_available = True
            self.gym_environment_name = self.gym_environment_name[3:]
            env = GymEnvironment(
                GymAvgTorqueWrapper(gym.make(self.gym_environment_name))
            )
        else:
            env = GymEnvironment(self.gym_environment_name, *self.args, **self.kwargs)
        agent = PearlAgent(
            policy_learner=TD3(
                state_dim=env.observation_space.shape[0],
                action_space=env.action_space,
                actor_hidden_dims=[256, 256],
                critic_hidden_dims=[256, 256],
                critic_learning_rate=3e-4,
                actor_learning_rate=3e-4,
                training_rounds=1,
                exploration_module=NormalDistributionExploration(
                    mean=0,
                    std_dev=0.1,
                ),
            ),
            replay_buffer=FIFOOffPolicyReplayBuffer(50000),
            device_id=self.device_id,
        )
        # Enable saving cost in replay buffer if cost is available
        agent.replay_buffer.has_cost_available = has_cost_available

        info = online_learning(
            agent,
            env,
            number_of_episodes=number_of_episodes,
            learn_after_episode=True,
            print_every_x_episodes=1,
        )
        return info[&#34;return&#34;]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pearl.utils.scripts.benchmark.Evaluation" href="#pearl.utils.scripts.benchmark.Evaluation">Evaluation</a></li>
<li>abc.ABC</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="pearl.utils.scripts.benchmark.Evaluation" href="#pearl.utils.scripts.benchmark.Evaluation">Evaluation</a></b></code>:
<ul class="hlist">
<li><code><a title="pearl.utils.scripts.benchmark.Evaluation.evaluate" href="#pearl.utils.scripts.benchmark.Evaluation.evaluate">evaluate</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pearl.utils.scripts" href="index.html">pearl.utils.scripts</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="pearl.utils.scripts.benchmark.collect_data" href="#pearl.utils.scripts.benchmark.collect_data">collect_data</a></code></li>
<li><code><a title="pearl.utils.scripts.benchmark.evaluate" href="#pearl.utils.scripts.benchmark.evaluate">evaluate</a></code></li>
<li><code><a title="pearl.utils.scripts.benchmark.generate_plots" href="#pearl.utils.scripts.benchmark.generate_plots">generate_plots</a></code></li>
<li><code><a title="pearl.utils.scripts.benchmark.main" href="#pearl.utils.scripts.benchmark.main">main</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pearl.utils.scripts.benchmark.Evaluation" href="#pearl.utils.scripts.benchmark.Evaluation">Evaluation</a></code></h4>
<ul class="">
<li><code><a title="pearl.utils.scripts.benchmark.Evaluation.evaluate" href="#pearl.utils.scripts.benchmark.Evaluation.evaluate">evaluate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pearl.utils.scripts.benchmark.PearlContinuousSAC" href="#pearl.utils.scripts.benchmark.PearlContinuousSAC">PearlContinuousSAC</a></code></h4>
</li>
<li>
<h4><code><a title="pearl.utils.scripts.benchmark.PearlDDPG" href="#pearl.utils.scripts.benchmark.PearlDDPG">PearlDDPG</a></code></h4>
</li>
<li>
<h4><code><a title="pearl.utils.scripts.benchmark.PearlDQN" href="#pearl.utils.scripts.benchmark.PearlDQN">PearlDQN</a></code></h4>
</li>
<li>
<h4><code><a title="pearl.utils.scripts.benchmark.PearlLSTMDQN" href="#pearl.utils.scripts.benchmark.PearlLSTMDQN">PearlLSTMDQN</a></code></h4>
</li>
<li>
<h4><code><a title="pearl.utils.scripts.benchmark.PearlPPO" href="#pearl.utils.scripts.benchmark.PearlPPO">PearlPPO</a></code></h4>
</li>
<li>
<h4><code><a title="pearl.utils.scripts.benchmark.PearlTD3" href="#pearl.utils.scripts.benchmark.PearlTD3">PearlTD3</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>