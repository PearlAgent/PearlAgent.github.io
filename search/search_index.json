{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Pearl's official website!","text":""},{"location":"#pearl-a-production-ready-reinforcement-learning-ai-agent-library","title":"Pearl - A Production-ready Reinforcement Learning AI Agent Library","text":""},{"location":"#proudly-brought-by-the-applied-reinforcement-learning-team-meta","title":"Proudly brought by the Applied Reinforcement Learning team @  Meta","text":"<ul> <li>v0.1 - Pearl beta-version is now released! Annoucements: Twitter Post, LinkedIn Post<ul> <li>Highlighted on Meta NeurIPS 2023 Official Website: Website</li> <li>Highlighted by AI at Meta official handle on Twitter and LinkedIn: Twitter Post, LinkedIn Post.</li> </ul> </li> </ul> <p>Github Repo: Link</p> <p>Our paper is ArXived at: Link</p> <p>Our NeurIPS 2023 Presentation Slides is released here.</p>"},{"location":"#overview","title":"Overview","text":"<p>Pearl is a new production-ready Reinforcement Learning AI agent library open-sourced by the Applied Reinforcement Learning team at Meta. Furthering our efforts on open AI innovation, Pearl enables researchers and practitioners to develop Reinforcement Learning AI agents. These AI agents prioritize cumulative long-term feedback over immediate feedback and can adapt to environments with limited observability, sparse feedback, and high stochasticity. We hope that Pearl offers the community a means to build state-of-the-art Reinforcement Learning AI agents that can adapt to a wide range of complex production environments.</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#installation","title":"Installation","text":"<p>To install Pearl, you can simply clone this repo and run <code>pip install</code> in editable mode. To successfully install, you need to have <code>pip</code> version \u2265 21.3, and <code>setuptools</code> version \u2265 64.</p> <pre><code>git clone https://github.com/facebookresearch/Pearl.git\ncd Pearl\npip install -e .\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>To kick off a Pearl agent with a classic reinforcement learning environment, here's a quick example.</p> <pre><code>from pearl.pearl_agent import PearlAgent\nfrom pearl.action_representation_modules.one_hot_action_representation_module import (\n    OneHotActionTensorRepresentationModule,\n)\nfrom pearl.policy_learners.sequential_decision_making.deep_q_learning import (\n    DeepQLearning,\n)\nfrom pearl.replay_buffers.sequential_decision_making.fifo_off_policy_replay_buffer import (\n    FIFOOffPolicyReplayBuffer,\n)\nfrom pearl.utils.instantiations.environments.gym_environment import GymEnvironment\n\nenv = GymEnvironment(\"CartPole-v1\")\n\nnum_actions = env.action_space.n\nagent = PearlAgent(\n    policy_learner=DeepQLearning(\n        state_dim=env.observation_space.shape[0],\n        action_space=env.action_space,\n        hidden_dims=[64, 64],\n        training_rounds=20,\n        action_representation_module=OneHotActionTensorRepresentationModule(\n            max_number_actions=num_actions\n        ),\n    ),\n    replay_buffer=FIFOOffPolicyReplayBuffer(10_000),\n)\n\nobservation, action_space = env.reset()\nagent.reset(observation, action_space)\ndone = False\nwhile not done:\n    action = agent.act(exploit=False)\n    action_result = env.step(action)\n    agent.observe(action_result)\n    agent.learn()\n    done = action_result.done\n</code></pre> <p>Users can replace the environment with any real-world problems.</p>"},{"location":"#tutorials","title":"Tutorials","text":"<ol> <li> <p>The first tutorial of Pearl focuses on recommender systems. We derived a small contrived recommender system environment using the MIND dataset (Wu et al. 2020). More details in https://github.com/facebookresearch/Pearl/tree/main/pearl/tutorials/single_item_recommender_system_example/demo.ipynb</p> </li> <li> <p>The second tutorial of Pearl focuses on contextual bandit algorithms and their implementation using Pearl library. We designed a contextual bandit environment based on UCI dataset and tested the performance of neural implementations of SquareCB, LinUCB, and LinTS. More details in https://github.com/facebookresearch/Pearl/tree/main/pearl/tutorials/contextual_bandits/contextual_bandits_tutorial.ipynb</p> </li> </ol> <p>More tutorials coming in 2024.</p>"},{"location":"#design-and-features","title":"Design and Features","text":"<p> Pearl was built with a modular design so that industry practitioners or academic researchers can select any subset and flexibly combine features below to construct a Pearl agent customized for their specific use cases. Pearl offers a diverse set of unique features for production environments, including dynamic action spaces, offline learning, intelligent neural exploration, safe decision making, history summarization, and data augmentation.</p> <p>You can find many Pearl agent candidates with mix-and-match set of reinforcement learning features in utils/scripts/benchmark_config.py</p>"},{"location":"#adoption-in-real-world-applications","title":"Adoption in Real-world Applications","text":"<p>Pearl is in progress supporting real-world applications, including recommender systems, auction bidding system and creative selection. Each of them requires a subset of features offered by Pearl. To visualize the subset of features used by each of the applications above, see the table below.  Pearl Features Recommender Systems Auction Bidding Creative Selection Policy Learning \u2705 \u2705 \u2705 Intelligent Exploration \u2705 \u2705 \u2705 Safety \u2705 History Summarization \u2705 Replay Buffer \u2705 \u2705 \u2705 Contextual Bandit \u2705 Offline RL \u2705 \u2705 Dynamic Action Space \u2705 \u2705 Large-scale Neural Network \u2705 <p></p>"},{"location":"#comparison-to-other-libraries","title":"Comparison to Other Libraries","text":"<p> Pearl Features Pearl ReAgent (Superseded by Pearl) RLLib SB3 Tianshou Dopamine Agent Modularity \u2705 \u274c \u274c \u274c \u274c \u274c Dynamic Action Space \u2705 \u2705 \u274c \u274c \u274c \u274c Offline RL \u2705 \u2705 \u2705 \u2705 \u2705 \u274c Intelligent Exploration \u2705 \u274c \u274c \u274c \u26aa (limited support) \u274c Contextual Bandit \u2705 \u2705 \u26aa (only linear support) \u274c \u274c \u274c Safe Decision Making \u2705 \u274c \u274c \u274c \u274c \u274c History Summarization \u2705 \u274c \u2705 \u274c \u26aa (requires modifying environment state) \u274c Data Augmented Replay Buffer \u2705 \u274c \u2705 \u2705 \u2705 \u274c <p></p>"},{"location":"#cite-us","title":"Cite Us","text":"<pre><code>@article{pearl2023paper,\n    title = {Pearl: A Production-ready Reinforcement Learning Agent},\n    author = {Zheqing Zhu, Rodrigo de Salvo Braz, Jalaj Bhandari, Daniel Jiang, Yi Wan, Yonathan Efroni, Ruiyang Xu, Liyuan Wang, Hongbo Guo, Alex Nikulkov, Dmytro Korenkevych, Urun Dogan, Frank Cheng, Zheng Wu, Wanqiao Xu},\n    eprint = {arXiv preprint arXiv:2312.03814},\n    year = {2023}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>Pearl is MIT licensed, as found in the LICENSE file.</p>"},{"location":"design/","title":"Design","text":""}]}