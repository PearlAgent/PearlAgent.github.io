<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>pearl.policy_learners.contextual_bandits.disjoint_bandit API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pearl.policy_learners.contextual_bandits.disjoint_bandit</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python3
# Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.

from typing import Any, Dict, List, Optional

import torch

from pearl.api.action import Action
from pearl.api.action_space import ActionSpace
from pearl.history_summarization_modules.history_summarization_module import (
    SubjectiveState,
)
from pearl.neural_networks.common.utils import ensemble_forward

from pearl.policy_learners.contextual_bandits.contextual_bandit_base import (
    ContextualBanditBase,
)
from pearl.policy_learners.exploration_modules.common.score_exploration_base import (
    ScoreExplorationBase,
)
from pearl.policy_learners.exploration_modules.exploration_module import (
    ExplorationModule,
)
from pearl.replay_buffers.transition import TransitionBatch
from pearl.utils.functional_utils.learning.action_utils import (
    concatenate_actions_to_state,
)
from pearl.utils.instantiations.spaces.discrete_action import DiscreteActionSpace


class DisjointBanditContainer(ContextualBanditBase):
    &#34;&#34;&#34;
    Wrapper for disjoint models with discrete (and usually small) action space.
    Each action has its own bandit model (can be based on UCB or Thompson Sampling).
    Using the Composite design pattern:
    https://refactoring.guru/design-patterns/composite
    &#34;&#34;&#34;

    def __init__(
        self,
        feature_dim: int,
        arm_bandits: List[ContextualBanditBase],
        exploration_module: ExplorationModule,
        training_rounds: int = 100,
        batch_size: int = 128,
        state_features_only: bool = False,
    ) -&gt; None:
        super(DisjointBanditContainer, self).__init__(
            feature_dim=feature_dim,
            exploration_module=exploration_module,
            training_rounds=training_rounds,
            batch_size=batch_size,
        )
        # Currently our disjoint LinUCB usecase only use LinearRegression
        self._arm_bandits: torch.nn.ModuleList = torch.nn.ModuleList(arm_bandits)
        self._n_arms: int = len(arm_bandits)
        self._state_features_only = state_features_only

    @property
    def n_arms(self) -&gt; int:
        return self._n_arms

    def _validate_batch(self, batch: TransitionBatch) -&gt; None:
        assert (
            batch.action.dtype == torch.long
        ), &#34;action must be torch.long type (index of arm)&#34;
        assert batch.action.min().item() &gt;= 0, &#34;action must be &gt;= 0&#34;
        assert (
            batch.action.max().item() &lt; self._n_arms
        ), &#34;action must be &lt; number of arms&#34;

    def _partition_batch_by_arm(self, batch: TransitionBatch) -&gt; List[TransitionBatch]:
        &#34;&#34;&#34;
        Break input batch down into per-arm batches based on action
        &#34;&#34;&#34;
        batches = []
        for arm in range(self.n_arms):
            # mask of observations for this arm
            # assume action indices
            mask = batch.action[:, 0] == arm
            if batch.state.ndim == 2:
                # shape: (batch_size, feature_size)
                # same features for all arms
                state = batch.state
            elif batch.state.ndim == 3:
                # shape: (batch_size, num_arms, feature_size)
                # different features for each arm
                assert (
                    batch.state.shape[1] == self.n_arms
                ), &#34;For 3D state, 2nd dimension must be equal to number of arms&#34;
                state = batch.state[:, arm, :]
            batches.append(
                TransitionBatch(
                    state=state[mask],
                    reward=batch.reward[mask],
                    weight=batch.weight[mask]
                    if batch.weight is not None
                    else torch.ones_like(mask, dtype=torch.float),
                    # empty action features since disjoint model used
                    # action as index of per-arm model
                    # if arms need different features, use 3D `state` instead
                    action=torch.empty(
                        int(mask.sum().item()),
                        0,
                        dtype=torch.float,
                        device=batch.device,
                    ),
                ).to(batch.device)
            )
        return batches

    def learn_batch(self, batch: TransitionBatch) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;
        action_idx determines which of the models the observation will be routed to.
        &#34;&#34;&#34;
        self._validate_batch(batch)

        arm_batches = self._partition_batch_by_arm(batch)
        returns = {}
        for i, (arm_bandit, arm_batch) in enumerate(
            zip(self._arm_bandits, arm_batches)
        ):
            if len(arm_batch) == 0:
                # skip updates if batch has no observations for this arm
                continue
            returns.update(
                {
                    f&#34;arm_{i}_{k}&#34;: v
                    for k, v in arm_bandit.learn_batch(arm_batch).items()
                }
            )
        return returns

    @property
    def models(self) -&gt; List[torch.nn.Module]:
        &#34;&#34;&#34;
        Get a list of models of each bandit
        &#34;&#34;&#34;
        return [bandit.model for bandit in self._arm_bandits]

    def act(
        self,
        subjective_state: SubjectiveState,
        available_action_space: ActionSpace,
        action_availability_mask: Optional[torch.Tensor] = None,
        exploit: bool = False,
    ) -&gt; Action:
        assert isinstance(available_action_space, DiscreteActionSpace)
        # (batch_size, action_count, feature_size)
        feature = concatenate_actions_to_state(
            subjective_state=subjective_state,
            action_space=available_action_space,
            state_features_only=self._state_features_only,
        )
        # (batch_size, action_count, feature_size)

        values = ensemble_forward(self.models, feature, use_for_loop=True)
        return self._exploration_module.act(
            subjective_state=feature,
            action_space=available_action_space,
            values=values,
            # pyre-fixme[6]: In call `ExplorationModule.act`, for argument
            # `representation`, expected `Optional[Module]` but got `List[Module]`.
            representation=self.models,
            action_availability_mask=action_availability_mask,
        )

    def get_scores(
        self,
        subjective_state: SubjectiveState,
        action_space: DiscreteActionSpace,
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        Returns:
            UCB scores when exploration module is UCB
            Shape is (batch, num_arms) or (num_arms,)
        &#34;&#34;&#34;
        exploration_module = self._exploration_module
        assert isinstance(exploration_module, ScoreExplorationBase)

        feature = concatenate_actions_to_state(
            subjective_state=subjective_state,
            action_space=action_space,
            state_features_only=self._state_features_only,
        )
        # (batch_size, action_count, feature_size)

        return exploration_module.get_scores(
            subjective_state=feature,
            values=ensemble_forward(self.models, feature, use_for_loop=True),
            action_space=action_space,
            representation=self.models,  # pyre-fixme[6]: unexpected type
        ).squeeze()

    @property
    def optimizer(self) -&gt; torch.optim.Optimizer:
        return self._optimizer</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pearl.policy_learners.contextual_bandits.disjoint_bandit.DisjointBanditContainer"><code class="flex name class">
<span>class <span class="ident">DisjointBanditContainer</span></span>
<span>(</span><span>feature_dim: int, arm_bandits: List[<a title="pearl.policy_learners.contextual_bandits.contextual_bandit_base.ContextualBanditBase" href="contextual_bandit_base.html#pearl.policy_learners.contextual_bandits.contextual_bandit_base.ContextualBanditBase">ContextualBanditBase</a>], exploration_module: <a title="pearl.policy_learners.exploration_modules.exploration_module.ExplorationModule" href="../exploration_modules/exploration_module.html#pearl.policy_learners.exploration_modules.exploration_module.ExplorationModule">ExplorationModule</a>, training_rounds: int = 100, batch_size: int = 128, state_features_only: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Wrapper for disjoint models with discrete (and usually small) action space.
Each action has its own bandit model (can be based on UCB or Thompson Sampling).
Using the Composite design pattern:
<a href="https://refactoring.guru/design-patterns/composite">https://refactoring.guru/design-patterns/composite</a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DisjointBanditContainer(ContextualBanditBase):
    &#34;&#34;&#34;
    Wrapper for disjoint models with discrete (and usually small) action space.
    Each action has its own bandit model (can be based on UCB or Thompson Sampling).
    Using the Composite design pattern:
    https://refactoring.guru/design-patterns/composite
    &#34;&#34;&#34;

    def __init__(
        self,
        feature_dim: int,
        arm_bandits: List[ContextualBanditBase],
        exploration_module: ExplorationModule,
        training_rounds: int = 100,
        batch_size: int = 128,
        state_features_only: bool = False,
    ) -&gt; None:
        super(DisjointBanditContainer, self).__init__(
            feature_dim=feature_dim,
            exploration_module=exploration_module,
            training_rounds=training_rounds,
            batch_size=batch_size,
        )
        # Currently our disjoint LinUCB usecase only use LinearRegression
        self._arm_bandits: torch.nn.ModuleList = torch.nn.ModuleList(arm_bandits)
        self._n_arms: int = len(arm_bandits)
        self._state_features_only = state_features_only

    @property
    def n_arms(self) -&gt; int:
        return self._n_arms

    def _validate_batch(self, batch: TransitionBatch) -&gt; None:
        assert (
            batch.action.dtype == torch.long
        ), &#34;action must be torch.long type (index of arm)&#34;
        assert batch.action.min().item() &gt;= 0, &#34;action must be &gt;= 0&#34;
        assert (
            batch.action.max().item() &lt; self._n_arms
        ), &#34;action must be &lt; number of arms&#34;

    def _partition_batch_by_arm(self, batch: TransitionBatch) -&gt; List[TransitionBatch]:
        &#34;&#34;&#34;
        Break input batch down into per-arm batches based on action
        &#34;&#34;&#34;
        batches = []
        for arm in range(self.n_arms):
            # mask of observations for this arm
            # assume action indices
            mask = batch.action[:, 0] == arm
            if batch.state.ndim == 2:
                # shape: (batch_size, feature_size)
                # same features for all arms
                state = batch.state
            elif batch.state.ndim == 3:
                # shape: (batch_size, num_arms, feature_size)
                # different features for each arm
                assert (
                    batch.state.shape[1] == self.n_arms
                ), &#34;For 3D state, 2nd dimension must be equal to number of arms&#34;
                state = batch.state[:, arm, :]
            batches.append(
                TransitionBatch(
                    state=state[mask],
                    reward=batch.reward[mask],
                    weight=batch.weight[mask]
                    if batch.weight is not None
                    else torch.ones_like(mask, dtype=torch.float),
                    # empty action features since disjoint model used
                    # action as index of per-arm model
                    # if arms need different features, use 3D `state` instead
                    action=torch.empty(
                        int(mask.sum().item()),
                        0,
                        dtype=torch.float,
                        device=batch.device,
                    ),
                ).to(batch.device)
            )
        return batches

    def learn_batch(self, batch: TransitionBatch) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;
        action_idx determines which of the models the observation will be routed to.
        &#34;&#34;&#34;
        self._validate_batch(batch)

        arm_batches = self._partition_batch_by_arm(batch)
        returns = {}
        for i, (arm_bandit, arm_batch) in enumerate(
            zip(self._arm_bandits, arm_batches)
        ):
            if len(arm_batch) == 0:
                # skip updates if batch has no observations for this arm
                continue
            returns.update(
                {
                    f&#34;arm_{i}_{k}&#34;: v
                    for k, v in arm_bandit.learn_batch(arm_batch).items()
                }
            )
        return returns

    @property
    def models(self) -&gt; List[torch.nn.Module]:
        &#34;&#34;&#34;
        Get a list of models of each bandit
        &#34;&#34;&#34;
        return [bandit.model for bandit in self._arm_bandits]

    def act(
        self,
        subjective_state: SubjectiveState,
        available_action_space: ActionSpace,
        action_availability_mask: Optional[torch.Tensor] = None,
        exploit: bool = False,
    ) -&gt; Action:
        assert isinstance(available_action_space, DiscreteActionSpace)
        # (batch_size, action_count, feature_size)
        feature = concatenate_actions_to_state(
            subjective_state=subjective_state,
            action_space=available_action_space,
            state_features_only=self._state_features_only,
        )
        # (batch_size, action_count, feature_size)

        values = ensemble_forward(self.models, feature, use_for_loop=True)
        return self._exploration_module.act(
            subjective_state=feature,
            action_space=available_action_space,
            values=values,
            # pyre-fixme[6]: In call `ExplorationModule.act`, for argument
            # `representation`, expected `Optional[Module]` but got `List[Module]`.
            representation=self.models,
            action_availability_mask=action_availability_mask,
        )

    def get_scores(
        self,
        subjective_state: SubjectiveState,
        action_space: DiscreteActionSpace,
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        Returns:
            UCB scores when exploration module is UCB
            Shape is (batch, num_arms) or (num_arms,)
        &#34;&#34;&#34;
        exploration_module = self._exploration_module
        assert isinstance(exploration_module, ScoreExplorationBase)

        feature = concatenate_actions_to_state(
            subjective_state=subjective_state,
            action_space=action_space,
            state_features_only=self._state_features_only,
        )
        # (batch_size, action_count, feature_size)

        return exploration_module.get_scores(
            subjective_state=feature,
            values=ensemble_forward(self.models, feature, use_for_loop=True),
            action_space=action_space,
            representation=self.models,  # pyre-fixme[6]: unexpected type
        ).squeeze()

    @property
    def optimizer(self) -&gt; torch.optim.Optimizer:
        return self._optimizer</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pearl.policy_learners.contextual_bandits.contextual_bandit_base.ContextualBanditBase" href="contextual_bandit_base.html#pearl.policy_learners.contextual_bandits.contextual_bandit_base.ContextualBanditBase">ContextualBanditBase</a></li>
<li><a title="pearl.policy_learners.policy_learner.PolicyLearner" href="../policy_learner.html#pearl.policy_learners.policy_learner.PolicyLearner">PolicyLearner</a></li>
<li>torch.nn.modules.module.Module</li>
<li>abc.ABC</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="pearl.policy_learners.contextual_bandits.disjoint_bandit.DisjointBanditContainer.models"><code class="name">var <span class="ident">models</span> : List[torch.nn.modules.module.Module]</code></dt>
<dd>
<div class="desc"><p>Get a list of models of each bandit</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def models(self) -&gt; List[torch.nn.Module]:
    &#34;&#34;&#34;
    Get a list of models of each bandit
    &#34;&#34;&#34;
    return [bandit.model for bandit in self._arm_bandits]</code></pre>
</details>
</dd>
<dt id="pearl.policy_learners.contextual_bandits.disjoint_bandit.DisjointBanditContainer.n_arms"><code class="name">var <span class="ident">n_arms</span> : int</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def n_arms(self) -&gt; int:
    return self._n_arms</code></pre>
</details>
</dd>
<dt id="pearl.policy_learners.contextual_bandits.disjoint_bandit.DisjointBanditContainer.optimizer"><code class="name">var <span class="ident">optimizer</span> : torch.optim.optimizer.Optimizer</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def optimizer(self) -&gt; torch.optim.Optimizer:
    return self._optimizer</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pearl.policy_learners.contextual_bandits.disjoint_bandit.DisjointBanditContainer.act"><code class="name flex">
<span>def <span class="ident">act</span></span>(<span>self, subjective_state: torch.Tensor, available_action_space: <a title="pearl.api.action_space.ActionSpace" href="../../api/action_space.html#pearl.api.action_space.ActionSpace">ActionSpace</a>, action_availability_mask: Optional[torch.Tensor] = None, exploit: bool = False) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def act(
    self,
    subjective_state: SubjectiveState,
    available_action_space: ActionSpace,
    action_availability_mask: Optional[torch.Tensor] = None,
    exploit: bool = False,
) -&gt; Action:
    assert isinstance(available_action_space, DiscreteActionSpace)
    # (batch_size, action_count, feature_size)
    feature = concatenate_actions_to_state(
        subjective_state=subjective_state,
        action_space=available_action_space,
        state_features_only=self._state_features_only,
    )
    # (batch_size, action_count, feature_size)

    values = ensemble_forward(self.models, feature, use_for_loop=True)
    return self._exploration_module.act(
        subjective_state=feature,
        action_space=available_action_space,
        values=values,
        # pyre-fixme[6]: In call `ExplorationModule.act`, for argument
        # `representation`, expected `Optional[Module]` but got `List[Module]`.
        representation=self.models,
        action_availability_mask=action_availability_mask,
    )</code></pre>
</details>
</dd>
<dt id="pearl.policy_learners.contextual_bandits.disjoint_bandit.DisjointBanditContainer.get_scores"><code class="name flex">
<span>def <span class="ident">get_scores</span></span>(<span>self, subjective_state: torch.Tensor, action_space: <a title="pearl.utils.instantiations.spaces.discrete_action.DiscreteActionSpace" href="../../utils/instantiations/spaces/discrete_action.html#pearl.utils.instantiations.spaces.discrete_action.DiscreteActionSpace">DiscreteActionSpace</a>) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><h2 id="returns">Returns</h2>
<p>UCB scores when exploration module is UCB
Shape is (batch, num_arms) or (num_arms,)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_scores(
    self,
    subjective_state: SubjectiveState,
    action_space: DiscreteActionSpace,
) -&gt; torch.Tensor:
    &#34;&#34;&#34;
    Returns:
        UCB scores when exploration module is UCB
        Shape is (batch, num_arms) or (num_arms,)
    &#34;&#34;&#34;
    exploration_module = self._exploration_module
    assert isinstance(exploration_module, ScoreExplorationBase)

    feature = concatenate_actions_to_state(
        subjective_state=subjective_state,
        action_space=action_space,
        state_features_only=self._state_features_only,
    )
    # (batch_size, action_count, feature_size)

    return exploration_module.get_scores(
        subjective_state=feature,
        values=ensemble_forward(self.models, feature, use_for_loop=True),
        action_space=action_space,
        representation=self.models,  # pyre-fixme[6]: unexpected type
    ).squeeze()</code></pre>
</details>
</dd>
<dt id="pearl.policy_learners.contextual_bandits.disjoint_bandit.DisjointBanditContainer.learn_batch"><code class="name flex">
<span>def <span class="ident">learn_batch</span></span>(<span>self, batch: <a title="pearl.replay_buffers.transition.TransitionBatch" href="../../replay_buffers/transition.html#pearl.replay_buffers.transition.TransitionBatch">TransitionBatch</a>) ‑> Dict[str, Any]</span>
</code></dt>
<dd>
<div class="desc"><p>action_idx determines which of the models the observation will be routed to.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def learn_batch(self, batch: TransitionBatch) -&gt; Dict[str, Any]:
    &#34;&#34;&#34;
    action_idx determines which of the models the observation will be routed to.
    &#34;&#34;&#34;
    self._validate_batch(batch)

    arm_batches = self._partition_batch_by_arm(batch)
    returns = {}
    for i, (arm_bandit, arm_batch) in enumerate(
        zip(self._arm_bandits, arm_batches)
    ):
        if len(arm_batch) == 0:
            # skip updates if batch has no observations for this arm
            continue
        returns.update(
            {
                f&#34;arm_{i}_{k}&#34;: v
                for k, v in arm_bandit.learn_batch(arm_batch).items()
            }
        )
    return returns</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="pearl.policy_learners.contextual_bandits.contextual_bandit_base.ContextualBanditBase" href="contextual_bandit_base.html#pearl.policy_learners.contextual_bandits.contextual_bandit_base.ContextualBanditBase">ContextualBanditBase</a></b></code>:
<ul class="hlist">
<li><code><a title="pearl.policy_learners.contextual_bandits.contextual_bandit_base.ContextualBanditBase.learn" href="../policy_learner.html#pearl.policy_learners.policy_learner.PolicyLearner.learn">learn</a></code></li>
<li><code><a title="pearl.policy_learners.contextual_bandits.contextual_bandit_base.ContextualBanditBase.preprocess_batch" href="../policy_learner.html#pearl.policy_learners.policy_learner.PolicyLearner.preprocess_batch">preprocess_batch</a></code></li>
<li><code><a title="pearl.policy_learners.contextual_bandits.contextual_bandit_base.ContextualBanditBase.reset" href="../policy_learner.html#pearl.policy_learners.policy_learner.PolicyLearner.reset">reset</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pearl.policy_learners.contextual_bandits" href="index.html">pearl.policy_learners.contextual_bandits</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pearl.policy_learners.contextual_bandits.disjoint_bandit.DisjointBanditContainer" href="#pearl.policy_learners.contextual_bandits.disjoint_bandit.DisjointBanditContainer">DisjointBanditContainer</a></code></h4>
<ul class="two-column">
<li><code><a title="pearl.policy_learners.contextual_bandits.disjoint_bandit.DisjointBanditContainer.act" href="#pearl.policy_learners.contextual_bandits.disjoint_bandit.DisjointBanditContainer.act">act</a></code></li>
<li><code><a title="pearl.policy_learners.contextual_bandits.disjoint_bandit.DisjointBanditContainer.get_scores" href="#pearl.policy_learners.contextual_bandits.disjoint_bandit.DisjointBanditContainer.get_scores">get_scores</a></code></li>
<li><code><a title="pearl.policy_learners.contextual_bandits.disjoint_bandit.DisjointBanditContainer.learn_batch" href="#pearl.policy_learners.contextual_bandits.disjoint_bandit.DisjointBanditContainer.learn_batch">learn_batch</a></code></li>
<li><code><a title="pearl.policy_learners.contextual_bandits.disjoint_bandit.DisjointBanditContainer.models" href="#pearl.policy_learners.contextual_bandits.disjoint_bandit.DisjointBanditContainer.models">models</a></code></li>
<li><code><a title="pearl.policy_learners.contextual_bandits.disjoint_bandit.DisjointBanditContainer.n_arms" href="#pearl.policy_learners.contextual_bandits.disjoint_bandit.DisjointBanditContainer.n_arms">n_arms</a></code></li>
<li><code><a title="pearl.policy_learners.contextual_bandits.disjoint_bandit.DisjointBanditContainer.optimizer" href="#pearl.policy_learners.contextual_bandits.disjoint_bandit.DisjointBanditContainer.optimizer">optimizer</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>